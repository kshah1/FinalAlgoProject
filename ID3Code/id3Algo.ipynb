{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karanshah/Library/CloudStorage/OneDrive-NortheasternUniversity/TotalAlgoNewNew/FinalCode/FinalAlgoProject/ID3Code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCartCode\u001b[m\u001b[m           \u001b[34mID3Code\u001b[m\u001b[m            README.md\n",
      "\u001b[34mData\u001b[m\u001b[m               \u001b[34mKaranDecisionTrees\u001b[m\u001b[m \u001b[34msanityCheckID3\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy(S) = - ∑ pᵢ * log₂(pᵢ) ; i = 1 to n\n",
    "#IG(S, A) = Entropy(S) - ∑((|Sᵥ| / |S|) * Entropy(Sᵥ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node class used within building the tree\n",
    "class Node:\n",
    "    def __init__(self, data, maxdepth):\n",
    "        self.data = data\n",
    "        self.maxdepth = maxdepth\n",
    "        self.key = None #splitting attribute\n",
    "        self.infogain = None\n",
    "        self.label = None #classifier\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.parent = None\n",
    "        self.value = None #attribute value (ex. 'y' or 'n')\n",
    "\n",
    "\n",
    "def entropy(labels):\n",
    "    totalNumofLabels = len(labels)\n",
    "#     if total <= 1:\n",
    "#         return 0\n",
    "    classes, countOfClasses = np.unique(labels, return_counts=True)\n",
    "    if len(classes) <= 1: #if arity of the labels is 1 then the entropy is 1\n",
    "        return 0\n",
    "    ##computation\n",
    "    probs = countOfClasses/totalNumofLabels\n",
    "    ent = 0\n",
    "    for p in probs:\n",
    "        ent += p*math.log2(p) #calculation used to determine the entropy of the labels     \n",
    "    return -1 * ent\n",
    "\n",
    "\n",
    "def mutualInfo(data, position):\n",
    "    ent = entropy(data[:,-1])\n",
    "    lengthOfValues = len(data[:, position])\n",
    "    classes, countOfClasses = np.unique(data[:, position], return_counts=True)\n",
    "    if len(classes) <= 1:\n",
    "        return 0\n",
    "    ##calculate probabilities used in conditional entropy\n",
    "    probs = countOfClasses/lengthOfValues\n",
    "    ##calculate specific conditional entropies\n",
    "    specCondEnts = []\n",
    "    for c in classes:\n",
    "        specCondEnt = 0\n",
    "        boolean = data[:, position] == c\n",
    "        subset = data[boolean]\n",
    "        lengthOfSubset = len(subset)\n",
    "        subsetClasses, subsetCountofClasses = np.unique(subset[:,-1], return_counts=True)\n",
    "        subsetClasses, subsetCountofClasses\n",
    "        probsOfSubset = subsetCountofClasses/lengthOfSubset\n",
    "        for p in probsOfSubset:\n",
    "            if p != 0:\n",
    "                specCondEnt += p*math.log2(p)\n",
    "            else:\n",
    "                specCondEnt += 0\n",
    "        specCondEnts.append(-1*specCondEnt)\n",
    "    #calculate conditional entropy\n",
    "    condEnt = sum(p*specCondEnts[idx] for idx, p in enumerate(probs))\n",
    "    #calculate info gain\n",
    "    infoGain = ent - condEnt\n",
    "    return infoGain\n",
    "\n",
    "\n",
    "#function used to split the data at each node\n",
    "def splitData(data, idx):\n",
    "    classes = np.unique(data[:,idx])\n",
    "    subsets = []\n",
    "    for c in classes:\n",
    "        boolean = data[:, idx] == c\n",
    "        subset = data[boolean]\n",
    "        subsets.append(subset)\n",
    "\n",
    "    return subsets   \n",
    "\n",
    "\n",
    "#function returns attribute that obtains the highest mutual information at each node\n",
    "def bestMutualInfo(data, attributes): #optimize this to work better\n",
    "    if len(attributes) == 1: #unnecessary\n",
    "        return attributes[0]\n",
    "    elif len(attributes) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        atts = attributes.copy()[:len(attributes)-1]\n",
    "        bestInfoGain = -np.inf\n",
    "        bestatt = None\n",
    "        for idx, att in enumerate(atts):\n",
    "#             idx = header.index(att)\n",
    "            if mutualInfo(data, idx) >= bestInfoGain: \n",
    "                bestInfoGain = mutualInfo(data, idx)\n",
    "#                 bestatt = header[idx]\n",
    "                bestatt = atts[idx]\n",
    "\n",
    "        return bestInfoGain, bestatt\n",
    "\n",
    "\n",
    "\n",
    "#need to pass last column which is the target variable # calculates the majority class for the labels\n",
    "def maj_classifer(data): \n",
    "    labels = data.copy()\n",
    "    classes, countOfClasses = np.unique(labels, return_counts=True)\n",
    "    if len(classes) == 1:\n",
    "        return classes[0]\n",
    "    counter = 0\n",
    "    maxValue = -np.inf\n",
    "    best_label = None\n",
    "    while counter < len(classes):\n",
    "        for idx, count in enumerate(countOfClasses):\n",
    "            if count >= maxValue:\n",
    "                maxValue = count\n",
    "                best_label = classes[idx]\n",
    "            counter += 1\n",
    "    return best_label\n",
    "\n",
    "#Recursive Algorithm to build the tree\n",
    "def buildTree(traindata,feats, maxdepth): \n",
    "    # maxdepth is 0, then return the majority output class\n",
    "    if maxdepth == 0:\n",
    "        return maj_classifer(traindata[:,-1]) \n",
    "    #maxdepth is limited by number of features\n",
    "    if maxdepth > len(feats) + 1: \n",
    "        maxdepth = len(feats) + 1\n",
    "    #create the root node with all the training data and an initial max depth\n",
    "    root = Node(traindata, maxdepth) \n",
    "    #calculate the information gain at that node and the attribute that best splits the data at that node\n",
    "    infoGainVal, bestAtt = bestMutualInfo(root.data, feats) \n",
    "    #base case\n",
    "    if infoGainVal <= 0: \n",
    "        root.key = 'leaf'\n",
    "        root.label = maj_classifer(root.data[:, -1])\n",
    "        return root\n",
    "    root.key = bestAtt\n",
    "    root.label = maj_classifer(root.data[:,-1])\n",
    "    root.infogain = infoGainVal\n",
    "    #split the root node data \n",
    "    rightData, leftData = splitData(root.data, feats.index(bestAtt))\n",
    "\n",
    "    #recurse to the left subtree\n",
    "    if maxdepth != 1 and root.left == None :\n",
    "        root.left = buildTree(leftData, feats, maxdepth - 1) \n",
    "        root.left.value = leftData[0,feats.index(bestAtt)]\n",
    "        root.left.parent = bestMutualInfo(root.data, feats)[1]\n",
    "\n",
    "    #recurse to the right subtree\n",
    "    if maxdepth != 1 and root.right == None :\n",
    "        root.right = buildTree(rightData, feats, maxdepth - 1) \n",
    "        root.right.value = rightData[0,feats.index(bestAtt)]\n",
    "        root.right.parent = bestMutualInfo(root.data, feats)[1]\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "#Use recursion to print the tree\n",
    "def printPreorder(root, classOne, classTwo, counter = 0):\n",
    "    if root:\n",
    "        classes = [classOne, classTwo]\n",
    "        _, countOfClasses = np.unique(root.data[:,-1], return_counts=True)\n",
    "        if counter == 0:\n",
    "            print('[{} {} /{} {}]\\n'.format(countOfClasses[0], classes[0], countOfClasses[1], classes[1]))\n",
    "        else:\n",
    "            if len(countOfClasses) == 2:\n",
    "                print('|'* counter + '{} = {}: [{} {} /{} {}] \\n'.format(root.parent, root.value, countOfClasses[0], classes[0], countOfClasses[1], classes[1]))\n",
    "            elif len(countOfClasses) == 1 and _ == classes[0]:\n",
    "                print('|' * counter +'{} = {}: [{} {} /{} {}] \\n'.format(root.parent, root.value, countOfClasses[0], classes[0], 0, classes[1]))\n",
    "            else:\n",
    "                print('|' * counter + '{} = {}: [{} {} /{} {}] \\n'.format(root.parent, root.value, 0, classes[0], countOfClasses[0], classes[1]))\n",
    "\n",
    "        # Then recur on left child\n",
    "        printPreorder(root.left, classes[0], classes[1], counter+1)\n",
    "        #Finally recur on right child\n",
    "        printPreorder(root.right, classes[0], classes[1], counter+1)\n",
    "\n",
    "\n",
    "\n",
    "#Recursive function that traverses the tree and return the prediction of the query\n",
    "def prediction(tree, feats, row, maxdepth, currentdepth=1):\n",
    "    #base case\n",
    "    if tree.key == 'leaf':\n",
    "        return tree.label\n",
    "    #base case\n",
    "    if maxdepth == currentdepth:\n",
    "        return tree.label\n",
    "    #recurse\n",
    "    if any(tree.key == feat for feat in feats):\n",
    "        idx = feats.index(tree.key)\n",
    "        if row[idx] == tree.left.value:\n",
    "            left = prediction(tree.left,feats,row, maxdepth, currentdepth + 1)\n",
    "            return left\n",
    "        if row[idx] == tree.right.value:\n",
    "            right = prediction(tree.right,feats,row, maxdepth, currentdepth + 1)\n",
    "            return right \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unacc is unacceptable\n",
    "#acc is acc\n",
    "df = pd.read_csv(\"../Data/car-data-train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 0] = df.iloc[:, 0].apply(lambda val: \"low\"  \n",
    "                                    if val == \"med\" \n",
    "                                    else \"high\" if val == \"vhigh\" \n",
    "                                    else val)\n",
    "\n",
    "df.iloc[:, 1] = df.iloc[:, 1].apply(lambda val: \"low\"  \n",
    "                                    if val == \"med\" \n",
    "                                    else \"high\" if val == \"vhigh\" \n",
    "                                    else val)\n",
    "\n",
    "#Created >2 doors into one bin and leave 2 doors as 1 bin #uneven feature binning\n",
    "#Check for other binning strategy\n",
    "df.iloc[:, 2] = df.iloc[:, 2].apply(lambda val: val  \n",
    "                                    if val == '2' \n",
    "                                    else '>2')\n",
    "\n",
    "\n",
    "#Integer Division\n",
    "FirstHalfIdx = df.iloc[:, 3][df.iloc[:, 3] == \"4\"].index[:len(df.iloc[:, 3][df.iloc[:, 3] == \"4\"].index)//2 + 1]\n",
    "SecondHalfIdx = df.iloc[:, 3][df.iloc[:, 3] == \"4\"].index[len(df.iloc[:, 3][df.iloc[:, 3] == \"4\"].index)//2 + 1:]\n",
    "\n",
    "#Number of person. Half of 4 in 2 and other half in more\n",
    "df.iloc[:, 3] = df.apply(lambda row: \"2\" if row.name in FirstHalfIdx \n",
    "                                         else \"more\" \n",
    "                                         if row.name in SecondHalfIdx \n",
    "                                         else row[3], axis=1)\n",
    "\n",
    "\n",
    "#Integer Division\n",
    "FirstHalfIdx = df.iloc[:, 4][df.iloc[:, 4] == \"med\"].index[:len(df.iloc[:, 4][df.iloc[:, 4] == \"med\"].index)//2 + 1]\n",
    "SecondHalfIdx = df.iloc[:, 4][df.iloc[:, 4] == \"med\"].index[len(df.iloc[:, 4][df.iloc[:, 4] == \"med\"].index)//2 + 1:]\n",
    "\n",
    "#Number of person. Half of 4 in 2 and other half in more\n",
    "df.iloc[:, 4] = df.apply(lambda row: \"small\" if row.name in FirstHalfIdx \n",
    "                                             else \"big\" \n",
    "                                             if row.name in SecondHalfIdx \n",
    "                                             else row[4], axis=1)\n",
    "\n",
    "#Integer Division\n",
    "FirstHalfIdx = df.iloc[:, 5][df.iloc[:, 5] == \"med\"].index[:len(df.iloc[:, 4][df.iloc[:, 4] == \"med\"].index)//2 + 1]\n",
    "SecondHalfIdx = df.iloc[:, 5][df.iloc[:, 5] == \"med\"].index[len(df.iloc[:, 4][df.iloc[:, 4] == \"med\"].index)//2 + 1:]\n",
    "\n",
    "#Number of person. Half of 4 in 2 and other half in more\n",
    "df.iloc[:, 5] = df.apply(lambda row: \"low\" if row.name in FirstHalfIdx \n",
    "                                             else \"high\" \n",
    "                                             if row.name in SecondHalfIdx \n",
    "                                             else row[5], axis=1)\n",
    "\n",
    "\n",
    "#Count the binnings\n",
    "value_counts = [df.iloc[:, i].value_counts() for i in range(len(df.columns))]\n",
    "\n",
    "df.columns = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"label\"]\n",
    "df.to_csv(\"train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataArr = df.to_numpy()\n",
    "\n",
    "X, y = dataArr[: , :len(df.columns) - 1], dataArr[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f86e2613bf16b7f48244979450dca00f7233b8c859298ca56e9835ce3672caba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
